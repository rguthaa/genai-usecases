{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chmod +x ../../../setup.sh\n",
    "#!cd ../../.. && bash setup.sh OpenAI\n",
    "#!pip install langchain-community\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install langchain_openai\n",
    "\n",
    "#!pip install langchain-postgres\n",
    "#!pip install \"unstructured[all-docs]\"\n",
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG pipeline for summarizing internal documentation\n",
    "\n",
    "Goal:\n",
    "1. Load the confluence documentation which are in markdown format\n",
    "2. Chunk and embed the confluence documentation\n",
    "3. Generate vector embeddings and store in vectordb\n",
    "4. Similarity search from vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and pre-process confluence markdown pages\n",
    "\n",
    "**Why should we clean before generating embeddings?**\n",
    "\n",
    "1. Remove Noise That Confuses the Model - LLMs are trained on well structured text, so these MD semantics doesn't make sense and also confuse which leads to in correct results\n",
    "2. Avoid Wasting Tokens - Markdown/HTML tags count as tokens when embedding — and most of the time they don’t add value.\n",
    "3. Improve Retrieval Quality - If you chunk and embed content with markdown noise, your RAG system will \n",
    "- Return less relevant results\n",
    "- Match on formatting, not concepts (e.g., # instead of “Login Flow”)\n",
    "- Generate awkward completions\n",
    "4. Better User Experience - Final response with markdown artifacts will confuse users and looks unpolished\n",
    "5. Avoid Chunking Mistakes - Markdown headers, code blocks, and lists can interfere with chunk splitting. Cleaning ensures each chunk is text-rich and meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean markdown content\n",
    "def clean_markdown(text: str) -> str:\n",
    "    text = re.sub(r'^#+\\s*', 'Section: ', text, flags=re.MULTILINE)  # headers\n",
    "    text = re.sub(r'[*_`]+', '', text)  # bold, italics, inline code\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)  # links\n",
    "    text = re.sub(r'^\\s*[-*+]\\s+', '- ', text, flags=re.MULTILINE)  # bullet points\n",
    "    text = re.sub(r'^\\s*\\d+\\.\\s+', '- ', text, flags=re.MULTILINE)  # numbered lists\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)  # extra newlines\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def process_md_file(file_path: str) -> list[Document]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_md = f.read()\n",
    "\n",
    "    cleaned_text = clean_markdown(raw_md)\n",
    "\n",
    "    # Step 3: Split using RecursiveCharacterTextSplitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"Section:\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.create_documents([cleaned_text])\n",
    "\n",
    "    # Step 4: Attach filename metadata\n",
    "    for doc in chunks:\n",
    "        doc.metadata[\"source\"] = file_path\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def process_md_folder(folder_path: str) -> list[Document]:\n",
    "    all_docs = []\n",
    "    for md_file in Path(folder_path).rglob(\"*.md\"):\n",
    "        docs = process_md_file(str(md_file))\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confluence_docs = process_md_folder(\"../../../data/confluence\")\n",
    "confluence_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and pre-process proprofs HTML pages\n",
    "\n",
    "- Parses full HTML\n",
    "- Extracts meaningful sections by header tags (h1, h2, h3)\n",
    "- Skips script, style tags etc.\n",
    "- Preserves document hierarchy\n",
    "- Adds metadata with header path\n",
    "\n",
    "You may need to write your own HTML preprocessor by using beautifulSoup if the HTML code has lots of custom tags specific to your app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def process_html_file(file_path: str) -> list[Document]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_md = f.read()\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"h1\", \"Section\"),\n",
    "        (\"h2\", \"Subsection\"),\n",
    "        (\"h3\", \"Detail\")\n",
    "    ]\n",
    "\n",
    "    splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    raw_chunks = splitter.split_text_from_file(file_path)\n",
    "    \n",
    "    proprofs_docs = [\n",
    "        Document(page_content=chunk.page_content, metadata={\"source\": file_path})\n",
    "        for chunk in raw_chunks\n",
    "    ]\n",
    "\n",
    "    return proprofs_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def process_html_folder(folder_path: str) -> list[Document]:\n",
    "    all_docs = []\n",
    "    for md_file in Path(folder_path).rglob(\"*.html\"):\n",
    "        docs = process_html_file(str(md_file))\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proprofs_docs = process_html_folder(\"../../../data/proprofs\")\n",
    "proprofs_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = confluence_docs + proprofs_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to use feature flags?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text Embeddings, Save to Vector DB and Similarity Search\n",
    "\n",
    "- Using OpenAI : Generate embeddings is not free, you need to have funded openai account and configure OpenAI key beforehand..\n",
    "- Using HuggingFace: Free\n",
    "\n",
    "If you are planning to use HF in production then you need to manage deployment and infra costs, but managed services like OpenAI you pay as you use and infra and deployment cost is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_embeddings_model_provider = \"HuggingFace\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using OpenAI - with 1536 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "if (use_embeddings_model_provider == 'OpenAI'):\n",
    "    openai_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    openai_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "if (use_embeddings_model_provider == 'OpenAI'):\n",
    "\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=all_docs,\n",
    "        embedding=openai_model,\n",
    "        collection_name=\"all_docs\",\n",
    "        persist_directory=\"../../chromadb_open_ai\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_embeddings_model_provider == 'OpenAI'):\n",
    "    results = vector_store.similarity_search(question, k=10)\n",
    "\n",
    "    seen_sources = set()\n",
    "    unique_docs = []\n",
    "\n",
    "    for doc in results:\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        if source and source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            unique_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using HuggingFace - with 384 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "if (use_embeddings_model_provider == 'HuggingFace'):\n",
    "    hf_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "if (use_embeddings_model_provider == 'HuggingFace'):\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=all_docs,\n",
    "        embedding=hf_model,\n",
    "        collection_name=\"all_docs\",\n",
    "        persist_directory=\"../../chromadb_huggingface\"\n",
    "    )\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_embeddings_model_provider == 'HuggingFace'):\n",
    "    results = vector_store.similarity_search(question, k=10)\n",
    "\n",
    "    seen_sources = set()\n",
    "    unique_docs = []\n",
    "\n",
    "    for doc in results:\n",
    "        source = doc.metadata.get(\"source\")\n",
    "        if source and source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            unique_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask GenAI to Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that summarizes internal documentation to answer user questions. Can you summarize in bullet points?\"),\n",
    "    (\"human\", \"Context:\\n{context}\"),\n",
    "    (\"human\", \"Question:\\n{question}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build context\n",
    "context = \"\\n\".join([doc.page_content for doc in unique_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)  # or your preferred LLM\n",
    "\n",
    "# Build prompt\n",
    "messages = template.format_messages(context=context, question=question)  # limit if needed\n",
    "\n",
    "# Run inference\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
